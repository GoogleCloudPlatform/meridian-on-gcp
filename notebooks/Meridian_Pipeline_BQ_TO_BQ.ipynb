{
  "cells": [
    {
      "cell_type": "code",
      "id": "ejOs6kAXYqHJs7RRBCQudRqI",
      "metadata": {
        "tags": [],
        "id": "ejOs6kAXYqHJs7RRBCQudRqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de38f87-e104-4860-a408-e6f7e03f08ae"
      },
      "source": [
        "!pip install kfp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kfp\n",
            "  Downloading kfp-2.12.1.tar.gz (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.8)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.0)\n",
            "Collecting kfp-pipeline-spec==0.6.0 (from kfp)\n",
            "  Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl.metadata (293 bytes)\n",
            "Collecting kfp-server-api<2.5.0,>=2.1.0 (from kfp)\n",
            "  Downloading kfp_server_api-2.4.0.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kubernetes<31,>=8.0.0 (from kfp)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (4.25.5)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.0.0)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Collecting urllib3<2.0.0 (from kfp)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.10)\n",
            "Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl (9.1 kB)\n",
            "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kfp, kfp-server-api\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-2.12.1-py3-none-any.whl size=366348 sha256=f2b91e845cb5e470b3ea1e1fdf4115b8347ee81dfc01502d7e08d38b859406ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a4/89/6f1fa2a3dae3976bc14d70e368e4064be8d4b0628af0ef7b85\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-2.4.0-py3-none-any.whl size=116526 sha256=cd4e7492643fb58490a284f11377f9b92b05c711a028a311e2ff1c28fe9291a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/0f/30/65abccda2186c59a28fd37c13675dca68d4e3b9e15f731107d\n",
            "Successfully built kfp kfp-server-api\n",
            "Installing collected packages: urllib3, kfp-pipeline-spec, kfp-server-api, kubernetes, kfp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "Successfully installed kfp-2.12.1 kfp-pipeline-spec-0.6.0 kfp-server-api-2.4.0 kubernetes-30.1.0 urllib3-1.26.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports and Config ---\n",
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.dsl import Input, Output, Model, Dataset, Artifact, OutputPath\n",
        "from typing import NamedTuple, Optional\n",
        "import google.cloud.aiplatform as aip\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import logging # Import logging\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = \"YOUR-PROJECT-ID\"\n",
        "REGION = \"us-central1\"\n",
        "PIPELINE_ROOT = \"gs://YOUR-BUCKET/meridian-pipeline-root\"\n",
        "BQ_DATASET = \"meridiansampledataset\" # Your dataset\n",
        "BQ_TABLE_NAME = \"meridiantable\" # your table name\n",
        "BQ_SUMMARY_TABLE_NAME = \"meridian_media_summary_report\" # Name for the new BQ out table\n",
        "OUTPUT_GCS_DIR = f\"{PIPELINE_ROOT}/outputs\"\n",
        "ROI_MU = 0.2\n",
        "ROI_SIGMA = 0.9\n",
        "N_CHAINS = 7\n",
        "N_ADAPT = 500\n",
        "N_BURNIN = 500\n",
        "N_KEEP = 1000\n",
        "RANDOM_SEED = 1\n",
        "REPORT_START_DATE = '2021-01-25'\n",
        "REPORT_END_DATE = '2024-01-15'\n",
        "STANDARD_BASE_IMAGE = \"python:3.10-slim\"\n",
        "GPU_BASE_IMAGE = \"gcr.io/deeplearning-platform-release/tf-gpu.2-15.py310\"\n",
        "MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "PIPELINE_NAME = \"meridian-mmm-gpu-bq-pipeline\" # pipeline name\n",
        "PIPELINE_JSON = f\"{PIPELINE_NAME}.json\"\n",
        "\n",
        "# --- Configure logging for components ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "# --- train_meridian_model ---\n",
        "@dsl.component(\n",
        "    base_image=GPU_BASE_IMAGE,\n",
        "    packages_to_install=[\n",
        "        \"google-meridian[and-cuda]\", \"numpy<2\",\"tensorflow_probability\", \"pandas\",\n",
        "        \"google-cloud-storage\", \"arviz\", \"matplotlib\", \"dill\",\n",
        "        \"google-cloud-bigquery\",\"db-dtypes\",\n",
        "        \"pyarrow\"\n",
        "    ],\n",
        ")\n",
        "def train_meridian_model(\n",
        "    project_id: str,\n",
        "    bq_dataset: str,\n",
        "    bq_table_name: str,\n",
        "    roi_mu: float, roi_sigma: float, n_chains: int,\n",
        "    n_adapt: int, n_burnin: int, n_keep: int, seed: int,\n",
        "    output_model: Output[Model],\n",
        "):\n",
        "    # --- Imports inside component ---\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_probability as tfp\n",
        "    import os\n",
        "    import logging\n",
        "    import time\n",
        "    import datetime\n",
        "    from google.cloud import bigquery\n",
        "    from meridian import constants\n",
        "    from meridian.data import load\n",
        "    from meridian.model import model, spec, prior_distribution\n",
        "    import dill\n",
        "\n",
        "    # --- Reconfigure logging inside component if needed, or rely on root config ---\n",
        "    # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Optional reconfig\n",
        "\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\" # Model Name\n",
        "\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        logging.info(f\"GPUs available: {gpus}\")\n",
        "        try:\n",
        "            for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logging.info(\"Enabled memory growth for GPUs.\")\n",
        "        except RuntimeError as e: logging.error(f\"Error setting memory growth: {e}\")\n",
        "    else: logging.warning(\"No GPU detected by TensorFlow. Running on CPU.\")\n",
        "\n",
        "    # --- Define Mappings ---\n",
        "    coord_to_columns = load.CoordToColumns(\n",
        "        time='time', geo='geo', controls=['GQV', 'Competitor_Sales'], population='population',\n",
        "        kpi='conversions', revenue_per_kpi='revenue_per_conversion',\n",
        "        media=[f'Channel{i}_impression' for i in range(5)], ## HERE FOR THE SAMPLE DATASET, change with your own channels names\n",
        "        media_spend=[f'Channel{i}_spend' for i in range(5)], ## HERE FOR THE SAMPLE DATASET, change with your own channels names\n",
        "        organic_media=['Organic_channel0_impression'], non_media_treatments=['Promo'],\n",
        "    )\n",
        "    correct_media_to_channel = {f'Channel{i}_impression': f'Channel_{i}' for i in range(5)} ## HERE FOR THE SAMPLE DATASET, change with your own channels names\n",
        "    correct_media_spend_to_channel = {f'Channel{i}_spend': f'Channel_{i}' for i in range(5)} ## HERE FOR THE SAMPLE DATASET, change with your own channels names\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # --- BigQuery Data Loading Start ---\n",
        "    bq_table_full_id = f\"{project_id}.{bq_dataset}.{bq_table_name}\"\n",
        "    logging.info(f\"Attempting to load data from BigQuery table: {bq_table_full_id}\")\n",
        "\n",
        "    try:\n",
        "        client = bigquery.Client(project=project_id)\n",
        "        logging.info(\"BigQuery client created successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to create BigQuery client: {e}\")\n",
        "        raise e\n",
        "\n",
        "    sql_query = f\"SELECT * FROM `{bq_table_full_id}`\"\n",
        "    logging.info(f\"Executing query: {sql_query}\")\n",
        "\n",
        "    try:\n",
        "        df = client.query(sql_query).to_dataframe()\n",
        "        logging.info(f\"Successfully loaded {len(df)} rows and {len(df.columns)} columns from BigQuery.\")\n",
        "\n",
        "        # --- Convert time column, BQ_To_Dataframe converts the datetime so we need to convert it yyyy-mm-dd ---\n",
        "        time_col_name = coord_to_columns.time\n",
        "        if time_col_name in df.columns:\n",
        "            logging.info(f\"Converting time column '{time_col_name}' to string format 'YYYY-MM-DD'\")\n",
        "            if pd.api.types.is_datetime64_any_dtype(df[time_col_name]) or isinstance(df[time_col_name].iloc[0], pd.Timestamp) or isinstance(df[time_col_name].iloc[0], datetime.date):\n",
        "                 df[time_col_name] = pd.to_datetime(df[time_col_name]).dt.strftime('%Y-%m-%d')\n",
        "                 logging.info(f\"Conversion of '{time_col_name}' complete.\")\n",
        "            elif pd.api.types.is_string_dtype(df[time_col_name]):\n",
        "                 logging.info(f\"Column '{time_col_name}' is already string type. Checking format (first row): {df[time_col_name].iloc[0]}\")\n",
        "            else:\n",
        "                 logging.warning(f\"Column '{time_col_name}' is not a recognized datetime or string type ({df[time_col_name].dtype}). Meridian might still fail.\")\n",
        "        else:\n",
        "            logging.error(f\"Specified time column '{time_col_name}' not found in DataFrame!\")\n",
        "            raise ValueError(f\"Time column '{time_col_name}' defined in coord_to_columns not found in BigQuery results.\")\n",
        "        # --- End Time Conversion ---\n",
        "\n",
        "        logging.info(\"First 5 rows of loaded data (post-conversion):\")\n",
        "        logging.info(df.head().to_string()) # Use to_string for logging DataFrames\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from BigQuery or processing DataFrame: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # --- Use DataFrameDataLoader ---\n",
        "    logging.info(\"Initializing Meridian DataFrameDataLoader...\")\n",
        "    try:\n",
        "        loader = load.DataFrameDataLoader(\n",
        "            df=df, # Pass the DataFrame loaded from BQ\n",
        "            kpi_type='non_revenue',\n",
        "            coord_to_columns=coord_to_columns,\n",
        "            media_to_channel=correct_media_to_channel,\n",
        "            media_spend_to_channel=correct_media_spend_to_channel,\n",
        "        )\n",
        "        data = loader.load()\n",
        "        logging.info(\"Data successfully loaded into Meridian InputData format.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during Meridian data loading process (DataFrameDataLoader): {e}\")\n",
        "        raise e\n",
        "    # --- BigQuery Data Loading End ---\n",
        "\n",
        "\n",
        "    logging.info(\"Configuring model...\")\n",
        "    prior = prior_distribution.PriorDistribution(\n",
        "        roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n",
        "    )\n",
        "    model_spec_obj = spec.ModelSpec(prior=prior)\n",
        "    mmm = model.Meridian(input_data=data, model_spec=model_spec_obj) # Use the 'data' object loaded from BQ\n",
        "\n",
        "    logging.info(\"Sampling prior...\")\n",
        "    mmm.sample_prior(500)\n",
        "    logging.info(f\"Sampling posterior with {n_chains} chains...\")\n",
        "    start_time = time.time()\n",
        "    mmm.sample_posterior(\n",
        "        n_chains=n_chains, n_adapt=n_adapt, n_burnin=n_burnin, n_keep=n_keep, seed=seed\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Posterior sampling complete. Duration: {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    save_file_path = os.path.join(output_model.path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Saving model artifact using model.save_mmm to file: {save_file_path}\")\n",
        "    try:\n",
        "        os.makedirs(output_model.path, exist_ok=True)\n",
        "        model.save_mmm(mmm, save_file_path)\n",
        "        logging.info(\"Model saved successfully using meridian.model.model.save_mmm.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"meridian.model.model.save_mmm failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    output_model.metadata[\"framework\"] = \"Meridian\"\n",
        "    output_model.metadata[\"saved_filename\"] = MERIDIAN_MODEL_FILENAME\n",
        "    output_model.metadata[\"description\"] = f\"Trained Meridian MMM model (BQ Input, saved via save_mmm to {MERIDIAN_MODEL_FILENAME})\"\n",
        "    logging.info(\"Training component finished.\")\n",
        "\n",
        "\n",
        "# --- generate_summary_report HTML ---\n",
        "@dsl.component(\n",
        "    base_image=STANDARD_BASE_IMAGE,\n",
        "    packages_to_install=[\n",
        "        \"google-meridian\",\n",
        "        \"tensorflow\", \"tensorflow_probability\",\n",
        "        \"pandas\", \"numpy\", \"arviz\", \"matplotlib\", \"google-cloud-storage\",\"dill\"\n",
        "    ],\n",
        ")\n",
        "def generate_summary_report(\n",
        "    model_artifact: Input[Model],\n",
        "    output_gcs_dir: str,\n",
        "    report_filename: str,\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    summary_report_artifact: Output[Artifact],\n",
        "):\n",
        "    import os\n",
        "    import logging\n",
        "    import time\n",
        "    import tempfile\n",
        "    from meridian.analysis import summarizer # Use summarizer for HTML report\n",
        "    from meridian.model import model\n",
        "    from google.cloud import storage\n",
        "    from urllib.parse import urlparse\n",
        "    import dill # Ensure dill is imported, needed by load_mmm\n",
        "\n",
        "    # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Optional reconfig\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "    def upload_local_file_to_gcs(local_path: str, gcs_uri: str):\n",
        "        storage_client = storage.Client()\n",
        "        parsed_uri = urlparse(gcs_uri)\n",
        "        bucket_name = parsed_uri.netloc\n",
        "        destination_blob_name = parsed_uri.path.lstrip('/')\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "        blob.upload_from_filename(local_path)\n",
        "        logging.info(f\"File {local_path} uploaded to {gcs_uri}\")\n",
        "\n",
        "    model_dir_path = model_artifact.path\n",
        "    load_file_path = os.path.join(model_dir_path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Attempting to load model from file: {load_file_path}\")\n",
        "    if not os.path.exists(load_file_path):\n",
        "        raise FileNotFoundError(f\"Expected model file {MERIDIAN_MODEL_FILENAME} not found in {model_dir_path}\")\n",
        "    try:\n",
        "        mmm = model.load_mmm(load_file_path)\n",
        "        logging.info(\"Model loaded successfully for HTML report generation.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model loading failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    if not output_gcs_dir.startswith(\"gs://\"):\n",
        "        raise ValueError(\"output_gcs_dir must be a GCS path (gs://...)\")\n",
        "    final_gcs_uri = os.path.join(output_gcs_dir, report_filename)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        logging.info(f\"Generating summary HTML report locally in: {temp_dir}\")\n",
        "        local_report_source_path = os.path.join(temp_dir, report_filename)\n",
        "        try:\n",
        "            # Use Summarizer for the HTML report as in original code\n",
        "            mmm_summarizer = summarizer.Summarizer(mmm)\n",
        "            mmm_summarizer.output_model_results_summary(\n",
        "                filename=report_filename,\n",
        "                filepath=temp_dir,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date\n",
        "            )\n",
        "            logging.info(f\"Meridian saved HTML report locally to: {local_report_source_path}\")\n",
        "            if not os.path.exists(local_report_source_path):\n",
        "                logging.error(f\"Meridian did not create the expected local HTML report file: {local_report_source_path}\")\n",
        "                raise FileNotFoundError(f\"HTML Report file not created locally by Meridian at {local_report_source_path}\")\n",
        "            logging.info(f\"Manually uploading {local_report_source_path} to {final_gcs_uri}\")\n",
        "            upload_local_file_to_gcs(local_report_source_path, final_gcs_uri)\n",
        "            summary_report_artifact.uri = final_gcs_uri\n",
        "            summary_report_artifact.metadata[\"gcs_path\"] = final_gcs_uri\n",
        "            summary_report_artifact.metadata[\"filename\"] = report_filename\n",
        "            logging.info(f\"Set KFP artifact URI for HTML report to: {summary_report_artifact.uri}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate or upload HTML summary report: {e}\")\n",
        "            raise e\n",
        "    logging.info(\"HTML Summary report component finished.\")\n",
        "\n",
        "\n",
        "# --- Generate and Save Summary Table to BigQuery ---\n",
        "@dsl.component(\n",
        "    base_image=STANDARD_BASE_IMAGE,\n",
        "    packages_to_install=[\n",
        "        \"google-meridian\", # Base meridian package\n",
        "        \"tensorflow\", \"tensorflow_probability\", # Dependencies for meridian\n",
        "        \"pandas\",\n",
        "        \"google-cloud-bigquery\",\n",
        "        \"pandas_gbq\",\n",
        "        \"google-cloud-storage\", # Needed for artifact loading\n",
        "        \"dill\", # For loading the model\n",
        "        \"pyarrow\",\n",
        "        \"db-dtypes\"\n",
        "    ]\n",
        ")\n",
        "def generate_and_save_summary_bq(\n",
        "    model_artifact: Input[Model],\n",
        "    project_id: str,\n",
        "    bq_dataset: str,\n",
        "    bq_table_name: str, # Target table for this summary\n",
        "    bq_output_table: Output[Artifact], # Output artifact to track the BQ table\n",
        "):\n",
        "    import os\n",
        "    import logging\n",
        "    import pandas_gbq\n",
        "    import pandas as pd\n",
        "    from meridian.analysis import visualizer # Use visualizer as per user image for the table\n",
        "    from meridian.model import model\n",
        "    from google.cloud import bigquery\n",
        "    import dill # Ensure dill is imported if needed by load_mmm\n",
        "\n",
        "    # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Optional reconfig\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "\n",
        "    model_dir_path = model_artifact.path\n",
        "    load_file_path = os.path.join(model_dir_path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Attempting to load model from file: {load_file_path} for BQ summary\")\n",
        "    if not os.path.exists(load_file_path):\n",
        "        raise FileNotFoundError(f\"Expected model file {MERIDIAN_MODEL_FILENAME} not found in {model_dir_path}\")\n",
        "\n",
        "    try:\n",
        "        mmm = model.load_mmm(load_file_path)\n",
        "        logging.info(\"Model loaded successfully for BQ summary generation.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model loading failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    logging.info(\"Generating media summary table using visualizer.MediaSummary...\")\n",
        "    try:\n",
        "        # Instantiate the visualizer's MediaSummary class\n",
        "        media_summary_visualizer = visualizer.MediaSummary(mmm)\n",
        "        summary_df = media_summary_visualizer.summary_table()\n",
        "        logging.info(\"Successfully generated summary DataFrame.\")\n",
        "        logging.info(\"First 5 rows of summary DataFrame:\")\n",
        "        logging.info(summary_df.head().to_string())\n",
        "        logging.info(\"\\nDataFrame Info:\")\n",
        "        # Use a StringIO buffer to capture info() output for logging\n",
        "        import io\n",
        "        buffer = io.StringIO()\n",
        "        summary_df.info(buf=buffer)\n",
        "        logging.info(buffer.getvalue())\n",
        "\n",
        "    except AttributeError:\n",
        "         logging.error(\"AttributeError: Could not find 'MediaSummary' or 'summary_table' in 'meridian.analysis.visualizer'. \"\n",
        "                       \"Perhaps the class/method name is different or in another module (e.g., summarizer)?\")\n",
        "         # --- Fallback attempt using Summarizer if Visualizer fails ---\n",
        "         logging.warning(\"Attempting fallback using meridian.analysis.summarizer.Summarizer...\")\n",
        "         try:\n",
        "             from meridian.analysis import summarizer\n",
        "             mmm_summarizer = summarizer.Summarizer(mmm)\n",
        "             if hasattr(mmm_summarizer, 'get_summary_dataframe'):\n",
        "                 summary_df = mmm_summarizer.get_summary_dataframe() # Hypothetical method\n",
        "                 logging.info(\"Successfully generated summary DataFrame using Summarizer fallback.\")\n",
        "             elif hasattr(mmm_summarizer, '_create_summary_table'): # Check private methods if desperate\n",
        "                  summary_df = mmm_summarizer._create_summary_table() # Highly discouraged, likely to break\n",
        "                  logging.info(\"Successfully generated summary DataFrame using Summarizer fallback (_create_summary_table).\")\n",
        "             else:\n",
        "                 logging.error(\"Fallback failed: Summarizer does not have a known method to return the summary DataFrame.\")\n",
        "                 raise ValueError(\"Could not generate summary DataFrame using known Meridian methods.\")\n",
        "         except Exception as fallback_e:\n",
        "             logging.error(f\"Error during Summarizer fallback: {fallback_e}\")\n",
        "             raise fallback_e # Re-raise the fallback error\n",
        "         # --- End Fallback attempt ---\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to generate summary table: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # --- Prepare DataFrame for BigQuery ---\n",
        "    # BQ prefers snake_case column names without special characters or spaces\n",
        "    original_columns = summary_df.columns.tolist()\n",
        "    new_columns = []\n",
        "    for col in original_columns:\n",
        "        new_col = str(col).lower() # Convert to string just in case, then lowercase\n",
        "        new_col = new_col.replace('% ', 'pct_').replace(' ', '_').replace('.', '').replace('(', '').replace(')', '')\n",
        "        new_columns.append(new_col)\n",
        "    summary_df.columns = new_columns\n",
        "    logging.info(f\"Renamed DataFrame columns for BQ compatibility: {new_columns}\")\n",
        "\n",
        "    # Convert complex object columns (like tuples represented as strings) to plain strings\n",
        "    # This prevents potential 'to_gbq' errors with complex types\n",
        "    for col in summary_df.columns:\n",
        "        if summary_df[col].dtype == 'object':\n",
        "            # Check if the first non-null value looks like a tuple/list string representation\n",
        "            first_val = summary_df[col].dropna().iloc[0] if not summary_df[col].dropna().empty else None\n",
        "            if isinstance(first_val, (tuple, list)) or (isinstance(first_val, str) and first_val.strip().startswith(('(', '['))):\n",
        "                 logging.info(f\"Converting object column '{col}' to string for BQ.\")\n",
        "                 summary_df[col] = summary_df[col].astype(str)\n",
        "            elif pd.api.types.is_numeric_dtype(summary_df[col].dropna()):\n",
        "                # Sometimes mixed types get 'object', try converting back to numeric if possible\n",
        "                 try:\n",
        "                     summary_df[col] = pd.to_numeric(summary_df[col])\n",
        "                     logging.info(f\"Converted object column '{col}' back to numeric.\")\n",
        "                 except: # Keep as object/string if conversion fails\n",
        "                     logging.warning(f\"Could not convert object column '{col}' to numeric, keeping as object/string.\")\n",
        "                     summary_df[col] = summary_df[col].astype(str) # Ensure string if not numeric\n",
        "            else: # Default to string conversion for other objects\n",
        "                 logging.info(f\"Converting object column '{col}' to string for BQ.\")\n",
        "                 summary_df[col] = summary_df[col].astype(str)\n",
        "\n",
        "\n",
        "    # Handle potential 'nan' strings from conversions if needed\n",
        "    summary_df = summary_df.fillna(pd.NA).replace(['nan', 'NaN', 'None', '(nan, nan)', 'nan (nan, nan)'], [pd.NA, pd.NA, pd.NA, pd.NA, pd.NA]) # Replace various nan strings with proper NA for BQ\n",
        "\n",
        "    # Reset index if it's meaningful (like the 0, 1, 2... row numbers) to make it a column\n",
        "    if summary_df.index.name is None and pd.api.types.is_integer_dtype(summary_df.index):\n",
        "         summary_df = summary_df.reset_index()\n",
        "         # Rename the new 'index' column if desired\n",
        "         index_col_name = 'original_index'\n",
        "         if index_col_name in summary_df.columns: # Avoid collision\n",
        "             index_col_name = 'row_index'\n",
        "         summary_df = summary_df.rename(columns={'index': index_col_name})\n",
        "         logging.info(f\"Reset DataFrame index and added column '{index_col_name}'.\")\n",
        "\n",
        "    logging.info(\"Final DataFrame Schema before BQ Upload:\")\n",
        "    buffer = io.StringIO()\n",
        "    summary_df.info(buf=buffer)\n",
        "    logging.info(buffer.getvalue())\n",
        "    logging.info(\"First 5 rows before BQ Upload:\")\n",
        "    logging.info(summary_df.head().to_string())\n",
        "\n",
        "\n",
        "    # --- Save to BigQuery ---\n",
        "    bq_table_full_id = f\"{project_id}.{bq_dataset}.{bq_table_name}\"\n",
        "    logging.info(f\"Attempting to save summary DataFrame to BigQuery table: {bq_table_full_id}\")\n",
        "\n",
        "    try:\n",
        "        client = bigquery.Client(project=project_id)\n",
        "        logging.info(\"BigQuery client created successfully.\")\n",
        "\n",
        "        # Use pandas_gbq or DataFrame.to_gbq (uses pandas_gbq backend)\n",
        "        summary_df.to_gbq(\n",
        "            destination_table=f\"{bq_dataset}.{bq_table_name}\",\n",
        "            project_id=project_id,\n",
        "            if_exists='replace', # Options: 'fail', 'replace', 'append'\n",
        "            # Optional: Define schema explicitly for more control if needed\n",
        "            # table_schema=[{'name': 'col1', 'type': 'STRING'}, ...]\n",
        "        )\n",
        "        logging.info(f\"Successfully wrote summary data to BigQuery table: {bq_table_full_id}\")\n",
        "\n",
        "        # Set output artifact metadata\n",
        "        bq_output_table.metadata[\"table_id\"] = bq_table_full_id\n",
        "        bq_output_table.uri = f\"https://console.cloud.google.com/bigquery?project={project_id}&ws=!1m5!1m4!4m3!1s{project_id}!2s{bq_dataset}!3s{bq_table_name}\" # URI to the BQ table\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to write DataFrame to BigQuery: {e}\")\n",
        "        # Log dataframe details that might cause issues\n",
        "        logging.error(f\"DataFrame dtypes:\\n{summary_df.dtypes}\")\n",
        "        raise e\n",
        "\n",
        "    logging.info(\"Generate and Save Summary to BQ component finished.\")\n",
        "\n",
        "\n",
        "# --- run_budget_optimization (Unchanged) ---\n",
        "@dsl.component(\n",
        "    base_image=STANDARD_BASE_IMAGE,\n",
        "    packages_to_install=[\n",
        "        \"google-meridian\",\n",
        "        \"pandas\", \"numpy\", \"google-cloud-storage\", \"dill\"\n",
        "    ],\n",
        ")\n",
        "def run_budget_optimization(\n",
        "    model_artifact: Input[Model],\n",
        "    output_gcs_dir: str,\n",
        "    report_filename: str,\n",
        "    optimization_report_artifact: Output[Artifact],\n",
        "):\n",
        "    # --- This component's *internal* code does not need to change ---\n",
        "    import os\n",
        "    import logging\n",
        "    import time\n",
        "    import tempfile\n",
        "    from meridian.analysis import optimizer\n",
        "    from meridian.model import model\n",
        "    from google.cloud import storage\n",
        "    from urllib.parse import urlparse\n",
        "    import dill # Ensure dill is imported if needed by load_mmm\n",
        "\n",
        "    # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Optional reconfig\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "    def upload_local_file_to_gcs(local_path: str, gcs_uri: str):\n",
        "        storage_client = storage.Client()\n",
        "        parsed_uri = urlparse(gcs_uri)\n",
        "        bucket_name = parsed_uri.netloc\n",
        "        destination_blob_name = parsed_uri.path.lstrip('/')\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "        blob.upload_from_filename(local_path)\n",
        "        logging.info(f\"File {local_path} uploaded to {gcs_uri}\")\n",
        "\n",
        "    model_dir_path = model_artifact.path\n",
        "    load_file_path = os.path.join(model_dir_path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Attempting to load model from file: {load_file_path}\")\n",
        "    if not os.path.exists(load_file_path):\n",
        "        raise FileNotFoundError(f\"Expected model file {MERIDIAN_MODEL_FILENAME} not found in {model_dir_path}\")\n",
        "    try:\n",
        "        mmm = model.load_mmm(load_file_path)\n",
        "        logging.info(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model loading failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    if not output_gcs_dir.startswith(\"gs://\"):\n",
        "        raise ValueError(\"output_gcs_dir must be a GCS path (gs://...)\")\n",
        "    final_gcs_uri = os.path.join(output_gcs_dir, report_filename)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        logging.info(f\"Running optimization and generating report locally in: {temp_dir}\")\n",
        "        local_report_source_path = os.path.join(temp_dir, report_filename)\n",
        "        try:\n",
        "            budget_optimizer = optimizer.BudgetOptimizer(mmm)\n",
        "            optimization_results = budget_optimizer.optimize()\n",
        "            logging.info(\"Optimization calculation complete.\")\n",
        "            optimization_results.output_optimization_summary(\n",
        "                filename=report_filename,\n",
        "                filepath=temp_dir\n",
        "            )\n",
        "            logging.info(f\"Meridian saved optimization report locally to: {local_report_source_path}\")\n",
        "            if not os.path.exists(local_report_source_path):\n",
        "                 logging.error(f\"Meridian did not create the expected local report file: {local_report_source_path}\")\n",
        "                 raise FileNotFoundError(f\"Optimization report file not created locally by Meridian at {local_report_source_path}\")\n",
        "            logging.info(f\"Manually uploading {local_report_source_path} to {final_gcs_uri}\")\n",
        "            upload_local_file_to_gcs(local_report_source_path, final_gcs_uri)\n",
        "            optimization_report_artifact.uri = final_gcs_uri\n",
        "            optimization_report_artifact.metadata[\"gcs_path\"] = final_gcs_uri\n",
        "            optimization_report_artifact.metadata[\"filename\"] = report_filename\n",
        "            logging.info(f\"Set KFP artifact URI to: {optimization_report_artifact.uri}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed during budget optimization or reporting/uploading: {e}\")\n",
        "            raise e\n",
        "    logging.info(\"Optimization component finished.\")\n",
        "\n",
        "\n",
        "# --- Pipeline Definition ---\n",
        "@dsl.pipeline(\n",
        "    name=PIPELINE_NAME,\n",
        "    description=\"Runs Meridian MMM (GPU) reading from BigQuery, saves summary table to BQ\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def meridian_pipeline(\n",
        "    project_id: str = PROJECT_ID,\n",
        "    bq_dataset: str = BQ_DATASET,\n",
        "    bq_table_name: str = BQ_TABLE_NAME, # Input data table\n",
        "    summary_bq_table_name: str = BQ_SUMMARY_TABLE_NAME, # Output summary table\n",
        "    output_gcs_dir: str = OUTPUT_GCS_DIR,\n",
        "    roi_mu: float = ROI_MU,\n",
        "    roi_sigma: float = ROI_SIGMA,\n",
        "    n_chains: int = N_CHAINS,\n",
        "    n_adapt: int = N_ADAPT,\n",
        "    n_burnin: int = N_BURNIN,\n",
        "    n_keep: int = N_KEEP,\n",
        "    seed: int = RANDOM_SEED,\n",
        "    report_start_date: str = REPORT_START_DATE,\n",
        "    report_end_date: str = REPORT_END_DATE,\n",
        "    summary_report_filename: str = \"summary_output.html\", # HTML report\n",
        "    optimization_report_filename: str = \"optimization_output.html\",\n",
        "):\n",
        "    # Step 1: Train Model\n",
        "    train_task = train_meridian_model(\n",
        "        project_id=project_id,\n",
        "        bq_dataset=bq_dataset,\n",
        "        bq_table_name=bq_table_name, # Input table\n",
        "        roi_mu=roi_mu, roi_sigma=roi_sigma,\n",
        "        n_chains=n_chains, n_adapt=n_adapt, n_burnin=n_burnin, n_keep=n_keep, seed=seed,\n",
        "    )\n",
        "    train_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\")\n",
        "    train_task.set_accelerator_limit(1).set_accelerator_type('NVIDIA_TESLA_T4')\n",
        "\n",
        "    # Step 2: Generate Summary Table and Save to BigQuery\n",
        "    save_summary_bq_task = generate_and_save_summary_bq(\n",
        "        model_artifact=train_task.outputs[\"output_model\"],\n",
        "        project_id=project_id,\n",
        "        bq_dataset=bq_dataset,\n",
        "        bq_table_name=summary_bq_table_name, # Output table name for summary\n",
        "    )\n",
        "    save_summary_bq_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\") # Adjust resources as needed\n",
        "\n",
        "    # Step 3: Generate HTML Summary Report (Runs in parallel with BQ save if desired, or after)\n",
        "    summary_html_task = generate_summary_report(\n",
        "        model_artifact=train_task.outputs[\"output_model\"],\n",
        "        output_gcs_dir=output_gcs_dir,\n",
        "        report_filename=summary_report_filename,\n",
        "        start_date=report_start_date,\n",
        "        end_date=report_end_date,\n",
        "    )\n",
        "    # Can run after BQ save by adding: .after(save_summary_bq_task)\n",
        "    summary_html_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\") # Keep original resources\n",
        "\n",
        "    # Step 4: Run Budget Optimization (Runs in parallel with reports if desired, or after)\n",
        "    optimization_task = run_budget_optimization(\n",
        "        model_artifact=train_task.outputs[\"output_model\"],\n",
        "        output_gcs_dir=output_gcs_dir,\n",
        "        report_filename=optimization_report_filename,\n",
        "    )\n",
        "    # Can run after reports by adding: .after(summary_html_task, save_summary_bq_task)\n",
        "    optimization_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\") # Keep original resources\n",
        "\n",
        "\n",
        "# --- Pipeline Compilation and Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    kfp.compiler.Compiler().compile(\n",
        "        pipeline_func=meridian_pipeline, package_path=PIPELINE_JSON\n",
        "    )\n",
        "    print(f\"Pipeline compiled to {PIPELINE_JSON}\")\n",
        "\n",
        "    aip.init(project=PROJECT_ID, location=REGION, staging_bucket=PIPELINE_ROOT)\n",
        "    print(f\"Initialized Vertex AI SDK for project {PROJECT_ID} in {REGION}\")\n",
        "\n",
        "    job = aip.PipelineJob(\n",
        "        display_name=PIPELINE_NAME, # Use updated name\n",
        "        template_path=PIPELINE_JSON,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        parameter_values={\n",
        "            \"project_id\": PROJECT_ID,\n",
        "            \"bq_dataset\": BQ_DATASET,\n",
        "            \"bq_table_name\": BQ_TABLE_NAME, # Input table\n",
        "            \"summary_bq_table_name\": BQ_SUMMARY_TABLE_NAME, # Output summary table\n",
        "            \"output_gcs_dir\": OUTPUT_GCS_DIR,\n",
        "            \"roi_mu\": ROI_MU,\n",
        "            \"roi_sigma\": ROI_SIGMA,\n",
        "            \"n_chains\": N_CHAINS,\n",
        "            \"n_adapt\": N_ADAPT,\n",
        "            \"n_burnin\": N_BURNIN,\n",
        "            \"n_keep\": N_KEEP,\n",
        "            \"seed\": RANDOM_SEED,\n",
        "            \"report_start_date\": REPORT_START_DATE,\n",
        "            \"report_end_date\": REPORT_END_DATE,\n",
        "            \"summary_report_filename\": \"summary_output.html\",\n",
        "            \"optimization_report_filename\": \"optimization_output.html\",\n",
        "        },\n",
        "        enable_caching=True, # Caching to edit as desired\n",
        "    )\n",
        "\n",
        "    print(\"Submitting pipeline job...\")\n",
        "    job.submit()\n",
        "    print(f\"Pipeline job submitted. View in Cloud Console: {job._dashboard_uri()}\")"
      ],
      "metadata": {
        "id": "CHhoN5Cu3TfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21001f8-6c4a-4976-8885-2ada23894ba8"
      },
      "id": "CHhoN5Cu3TfZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline compiled to meridian-mmm-gpu-bq-pipeline-v2.json\n",
            "Initialized Vertex AI SDK for project cloud-llm-preview2 in us-central1\n",
            "Submitting pipeline job...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/323656405210/locations/us-central1/pipelineJobs/meridian-mmm-gpu-bq-pipeline-v2-20250402170850\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/323656405210/locations/us-central1/pipelineJobs/meridian-mmm-gpu-bq-pipeline-v2-20250402170850')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/meridian-mmm-gpu-bq-pipeline-v2-20250402170850?project=323656405210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline job submitted. View in Cloud Console: https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/meridian-mmm-gpu-bq-pipeline-v2-20250402170850?project=323656405210\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Meridian-Pipeline-BQ-TO-BQ"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}