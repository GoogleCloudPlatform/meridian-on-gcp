{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Copyright 2025 Google LLC\n",
        "##\n",
        "## Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "## you may not use this file except in compliance with the License.\n",
        "## You may obtain a copy of the License at\n",
        "##\n",
        "##   http://www.apache.org/licenses/LICENSE-2.0\n",
        "##\n",
        "## Unless required by applicable law or agreed to in writing, software\n",
        "## distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "## See the License for the specific language governing permissions and\n",
        "## limitations under the License.\n",
        "##"
      ],
      "metadata": {
        "id": "IusSB40n8y0M"
      },
      "id": "IusSB40n8y0M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "ejOs6kAXYqHJs7RRBCQudRqI",
      "metadata": {
        "tags": [],
        "id": "ejOs6kAXYqHJs7RRBCQudRqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90da725d-44ec-4c4d-c61d-7989ac9c2d7a"
      },
      "source": [
        "!pip install kfp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kfp\n",
            "  Downloading kfp-2.12.1.tar.gz (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.8)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.0)\n",
            "Collecting kfp-pipeline-spec==0.6.0 (from kfp)\n",
            "  Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl.metadata (293 bytes)\n",
            "Collecting kfp-server-api<2.5.0,>=2.1.0 (from kfp)\n",
            "  Downloading kfp_server_api-2.4.0.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kubernetes<31,>=8.0.0 (from kfp)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (4.25.5)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.0.0)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Collecting urllib3<2.0.0 (from kfp)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (1.6.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.10)\n",
            "Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl (9.1 kB)\n",
            "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kfp, kfp-server-api\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-2.12.1-py3-none-any.whl size=366348 sha256=a0d8ef250ae2cf24e6b23d3df5089bc5cc337afcc24093da53f395884256493d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a4/89/6f1fa2a3dae3976bc14d70e368e4064be8d4b0628af0ef7b85\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-2.4.0-py3-none-any.whl size=116526 sha256=bf511cea4a4c02b4445adf31b2b269dc67950573e78a74bf7b75e41719b64359\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/0f/30/65abccda2186c59a28fd37c13675dca68d4e3b9e15f731107d\n",
            "Successfully built kfp kfp-server-api\n",
            "Installing collected packages: urllib3, kfp-pipeline-spec, kfp-server-api, kubernetes, kfp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "Successfully installed kfp-2.12.1 kfp-pipeline-spec-0.6.0 kfp-server-api-2.4.0 kubernetes-30.1.0 urllib3-1.26.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports and Config ---\n",
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.dsl import Input, Output, Model, Dataset, Artifact, OutputPath\n",
        "from typing import NamedTuple, Optional\n",
        "import google.cloud.aiplatform as aip\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = \"my-project-id\"\n",
        "REGION = \"us-central1\"\n",
        "PIPELINE_ROOT = \"gs://my-bucket/meridian-pipeline-root\"\n",
        "BQ_DATASET = \"meridiansampledataset\" # Dataset name in BQ\n",
        "BQ_TABLE_NAME = \"meridiantable\" # Table name in BQ\n",
        "OUTPUT_GCS_DIR = f\"{PIPELINE_ROOT}/outputs\"\n",
        "ROI_MU = 0.2\n",
        "ROI_SIGMA = 0.9\n",
        "N_CHAINS = 7\n",
        "N_ADAPT = 500\n",
        "N_BURNIN = 500\n",
        "N_KEEP = 1000\n",
        "RANDOM_SEED = 1\n",
        "REPORT_START_DATE = '2021-01-25'\n",
        "REPORT_END_DATE = '2024-01-15'\n",
        "STANDARD_BASE_IMAGE = \"python:3.10-slim\"\n",
        "GPU_BASE_IMAGE = \"gcr.io/deeplearning-platform-release/tf-gpu.2-15.py310\"\n",
        "MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "PIPELINE_NAME = \"meridian-mmm-gpu-bq-pipeline-v1\" # Give it a new version name\n",
        "PIPELINE_JSON = f\"{PIPELINE_NAME}.json\"\n",
        "\n",
        "# --- train_meridian_model ---\n",
        "@dsl.component(\n",
        "    base_image=GPU_BASE_IMAGE,\n",
        "    packages_to_install=[\n",
        "        \"google-meridian[and-cuda]\", \"numpy<2\",\"tensorflow_probability\", \"pandas\",\n",
        "        \"google-cloud-storage\", \"arviz\", \"matplotlib\", \"dill\",\n",
        "        \"google-cloud-bigquery\",\"db-dtypes\",\n",
        "        \"pyarrow\" # Often needed by BQ client's to_dataframe()\n",
        "    ],\n",
        ")\n",
        "def train_meridian_model(\n",
        "    project_id: str,\n",
        "    bq_dataset: str,\n",
        "    bq_table_name: str,\n",
        "    roi_mu: float, roi_sigma: float, n_chains: int,\n",
        "    n_adapt: int, n_burnin: int, n_keep: int, seed: int,\n",
        "    output_model: Output[Model],\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_probability as tfp\n",
        "    import os\n",
        "    import logging\n",
        "    import time\n",
        "    import datetime\n",
        "    from google.cloud import bigquery\n",
        "    from meridian import constants\n",
        "    from meridian.data import load\n",
        "    from meridian.model import model, spec, prior_distribution\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\" # Define inside component too\n",
        "\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        logging.info(f\"GPUs available: {gpus}\")\n",
        "        try:\n",
        "            for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logging.info(\"Enabled memory growth for GPUs.\")\n",
        "        except RuntimeError as e: logging.error(f\"Error setting memory growth: {e}\")\n",
        "    else: logging.warning(\"No GPU detected by TensorFlow. Running on CPU.\")\n",
        "\n",
        "    # --- Define Mappings ---\n",
        "    coord_to_columns = load.CoordToColumns(\n",
        "        time='time', geo='geo', controls=['GQV', 'Competitor_Sales'], population='population',\n",
        "        kpi='conversions', revenue_per_kpi='revenue_per_conversion',\n",
        "        media=[f'Channel{i}_impression' for i in range(5)], ## HERE FOR THE SAMPLE DATASET\n",
        "        media_spend=[f'Channel{i}_spend' for i in range(5)], ## HERE FOR THE SAMPLE DATASET\n",
        "        organic_media=['Organic_channel0_impression'], non_media_treatments=['Promo'],\n",
        "    )\n",
        "    correct_media_to_channel = {f'Channel{i}_impression': f'Channel_{i}' for i in range(5)}\n",
        "    correct_media_spend_to_channel = {f'Channel{i}_spend': f'Channel_{i}' for i in range(5)}\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # --- BigQuery Data Loading Start ---\n",
        "    bq_table_full_id = f\"{project_id}.{bq_dataset}.{bq_table_name}\"\n",
        "    logging.info(f\"Attempting to load data from BigQuery table: {bq_table_full_id}\")\n",
        "\n",
        "    try:\n",
        "        client = bigquery.Client(project=project_id)\n",
        "        logging.info(\"BigQuery client created successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to create BigQuery client: {e}\")\n",
        "        raise e\n",
        "\n",
        "    sql_query = f\"SELECT * FROM `{bq_table_full_id}`\"\n",
        "    logging.info(f\"Executing query: {sql_query}\")\n",
        "\n",
        "    try:\n",
        "        df = client.query(sql_query).to_dataframe()\n",
        "        logging.info(f\"Successfully loaded {len(df)} rows and {len(df.columns)} columns from BigQuery.\")\n",
        "\n",
        "        # --- Convert time column, BQ To Dataframe converts the datetime so we need to convert it yyyy-mm-dd ---\n",
        "        time_col_name = coord_to_columns.time\n",
        "        if time_col_name in df.columns:\n",
        "            logging.info(f\"Converting time column '{time_col_name}' to string format 'YYYY-MM-DD'\")\n",
        "            if pd.api.types.is_datetime64_any_dtype(df[time_col_name]) or isinstance(df[time_col_name].iloc[0], pd.Timestamp) or isinstance(df[time_col_name].iloc[0], datetime.date):\n",
        "                 df[time_col_name] = pd.to_datetime(df[time_col_name]).dt.strftime('%Y-%m-%d')\n",
        "                 logging.info(f\"Conversion of '{time_col_name}' complete.\")\n",
        "                 # logging.info(\"\\nDataFrame Info (after time conversion):\") # Optional detailed log\n",
        "                 # df.info(verbose=True, buf=open(os.devnull, 'w')) # Log info without printing to stdout\n",
        "            elif pd.api.types.is_string_dtype(df[time_col_name]):\n",
        "                 logging.info(f\"Column '{time_col_name}' is already string type. Checking format (first row): {df[time_col_name].iloc[0]}\")\n",
        "            else:\n",
        "                 logging.warning(f\"Column '{time_col_name}' is not a recognized datetime or string type ({df[time_col_name].dtype}). Meridian might still fail.\")\n",
        "        else:\n",
        "            logging.error(f\"Specified time column '{time_col_name}' not found in DataFrame!\")\n",
        "            raise ValueError(f\"Time column '{time_col_name}' defined in coord_to_columns not found in BigQuery results.\")\n",
        "        # --- End Time Conversion ---\n",
        "\n",
        "        # --- Optional Data Validation ---\n",
        "        logging.info(\"First 5 rows of loaded data (post-conversion):\")\n",
        "        logging.info(df.head().to_string()) # Use to_string for logging DataFrames\n",
        "        # Add required_cols check if desired, using logging.warning or logging.error\n",
        "        # --- End Validation ---\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from BigQuery or processing DataFrame: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # --- Use DataFrameDataLoader ---\n",
        "    logging.info(\"Initializing Meridian DataFrameDataLoader...\")\n",
        "    try:\n",
        "        loader = load.DataFrameDataLoader(\n",
        "            df=df, # Pass the DataFrame loaded from BQ\n",
        "            kpi_type='non_revenue', # Assuming this is still correct\n",
        "            coord_to_columns=coord_to_columns,\n",
        "            media_to_channel=correct_media_to_channel,\n",
        "            media_spend_to_channel=correct_media_spend_to_channel,\n",
        "        )\n",
        "        data = loader.load()\n",
        "        logging.info(\"Data successfully loaded into Meridian InputData format.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during Meridian data loading process (DataFrameDataLoader): {e}\")\n",
        "        raise e\n",
        "    # --- BigQuery Data Loading End ---\n",
        "\n",
        "\n",
        "    logging.info(\"Configuring model...\")\n",
        "    prior = prior_distribution.PriorDistribution(\n",
        "        roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n",
        "    )\n",
        "    model_spec_obj = spec.ModelSpec(prior=prior)\n",
        "    mmm = model.Meridian(input_data=data, model_spec=model_spec_obj) # Use the 'data' object loaded from BQ\n",
        "\n",
        "    logging.info(\"Sampling prior...\")\n",
        "    mmm.sample_prior(500)\n",
        "    logging.info(f\"Sampling posterior with {n_chains} chains...\")\n",
        "    start_time = time.time()\n",
        "    mmm.sample_posterior(\n",
        "        n_chains=n_chains, n_adapt=n_adapt, n_burnin=n_burnin, n_keep=n_keep, seed=seed\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    logging.info(f\"Posterior sampling complete. Duration: {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    save_file_path = os.path.join(output_model.path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Saving model artifact using model.save_mmm to file: {save_file_path}\")\n",
        "    try:\n",
        "        os.makedirs(output_model.path, exist_ok=True)\n",
        "        model.save_mmm(mmm, save_file_path)\n",
        "        logging.info(\"Model saved successfully using meridian.model.model.save_mmm.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"meridian.model.model.save_mmm failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    output_model.metadata[\"framework\"] = \"Meridian\"\n",
        "    output_model.metadata[\"saved_filename\"] = MERIDIAN_MODEL_FILENAME\n",
        "    output_model.metadata[\"description\"] = f\"Trained Meridian MMM model (BQ Input, saved via save_mmm to {MERIDIAN_MODEL_FILENAME})\"\n",
        "    logging.info(\"Training component finished.\")\n",
        "\n",
        "\n",
        "# --- Other components (generate_summary_report, run_budget_optimization) remain unchanged ---\n",
        "@dsl.component(\n",
        "    base_image=STANDARD_BASE_IMAGE,\n",
        "    packages_to_install=[\n",
        "        \"google-meridian[and-cuda]\", \"tensorflow\", \"tensorflow_probability\",\n",
        "        \"pandas\", \"numpy\", \"arviz\", \"matplotlib\", \"google-cloud-storage\",\"dill\"\n",
        "    ],\n",
        ")\n",
        "def generate_summary_report(\n",
        "    model_artifact: Input[Model],\n",
        "    output_gcs_dir: str,\n",
        "    report_filename: str,\n",
        "    start_date: str,\n",
        "    end_date: str,\n",
        "    summary_report_artifact: Output[Artifact],\n",
        "):\n",
        "    # --- This component's *internal* code does not need to change ---\n",
        "    # It loads the model artifact produced by the previous step.\n",
        "    import os\n",
        "    import logging\n",
        "    import time\n",
        "    import tempfile\n",
        "    from meridian.analysis import summarizer\n",
        "    from meridian.model import model\n",
        "    from google.cloud import storage\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "    def upload_local_file_to_gcs(local_path: str, gcs_uri: str):\n",
        "        storage_client = storage.Client()\n",
        "        parsed_uri = urlparse(gcs_uri)\n",
        "        bucket_name = parsed_uri.netloc\n",
        "        destination_blob_name = parsed_uri.path.lstrip('/')\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "        blob.upload_from_filename(local_path)\n",
        "        logging.info(f\"File {local_path} uploaded to {gcs_uri}\")\n",
        "\n",
        "    model_dir_path = model_artifact.path\n",
        "    load_file_path = os.path.join(model_dir_path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Attempting to load model from file: {load_file_path}\")\n",
        "    if not os.path.exists(load_file_path):\n",
        "        raise FileNotFoundError(f\"Expected model file {MERIDIAN_MODEL_FILENAME} not found in {model_dir_path}\")\n",
        "    try:\n",
        "        mmm = model.load_mmm(load_file_path)\n",
        "        logging.info(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model loading failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    if not output_gcs_dir.startswith(\"gs://\"):\n",
        "        raise ValueError(\"output_gcs_dir must be a GCS path (gs://...)\")\n",
        "    final_gcs_uri = os.path.join(output_gcs_dir, report_filename)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        logging.info(f\"Generating summary report locally in: {temp_dir}\")\n",
        "        local_report_source_path = os.path.join(temp_dir, report_filename)\n",
        "        try:\n",
        "            mmm_summarizer = summarizer.Summarizer(mmm)\n",
        "            mmm_summarizer.output_model_results_summary(\n",
        "                filename=report_filename,\n",
        "                filepath=temp_dir,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date\n",
        "            )\n",
        "            logging.info(f\"Meridian saved report locally to: {local_report_source_path}\")\n",
        "            if not os.path.exists(local_report_source_path):\n",
        "                logging.error(f\"Meridian did not create the expected local report file: {local_report_source_path}\")\n",
        "                raise FileNotFoundError(f\"Report file not created locally by Meridian at {local_report_source_path}\")\n",
        "            logging.info(f\"Manually uploading {local_report_source_path} to {final_gcs_uri}\")\n",
        "            upload_local_file_to_gcs(local_report_source_path, final_gcs_uri)\n",
        "            summary_report_artifact.uri = final_gcs_uri\n",
        "            summary_report_artifact.metadata[\"gcs_path\"] = final_gcs_uri\n",
        "            summary_report_artifact.metadata[\"filename\"] = report_filename\n",
        "            logging.info(f\"Set KFP artifact URI to: {summary_report_artifact.uri}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate or upload summary report: {e}\")\n",
        "            raise e\n",
        "    logging.info(\"Summary report component finished.\")\n",
        "\n",
        "\n",
        "@dsl.component(\n",
        "    base_image=STANDARD_BASE_IMAGE,  # CPU\n",
        "    packages_to_install=[\n",
        "        \"google-meridian\", # Removed cuda variant if not needed\n",
        "        \"pandas\", \"numpy\", \"google-cloud-storage\", \"dill\"\n",
        "    ],\n",
        ")\n",
        "def run_budget_optimization(\n",
        "    model_artifact: Input[Model],\n",
        "    output_gcs_dir: str,\n",
        "    report_filename: str,\n",
        "    optimization_report_artifact: Output[Artifact],\n",
        "):\n",
        "    # --- This component's *internal* code does not need to change ---\n",
        "    # It loads the model artifact produced by the training step.\n",
        "    import os\n",
        "    import logging\n",
        "    import time\n",
        "    import tempfile\n",
        "    from meridian.analysis import optimizer\n",
        "    from meridian.model import model\n",
        "    from google.cloud import storage\n",
        "    from urllib.parse import urlparse\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    MERIDIAN_MODEL_FILENAME = \"model_save.pkl\"\n",
        "    def upload_local_file_to_gcs(local_path: str, gcs_uri: str):\n",
        "        storage_client = storage.Client()\n",
        "        parsed_uri = urlparse(gcs_uri)\n",
        "        bucket_name = parsed_uri.netloc\n",
        "        destination_blob_name = parsed_uri.path.lstrip('/')\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "        blob.upload_from_filename(local_path)\n",
        "        logging.info(f\"File {local_path} uploaded to {gcs_uri}\")\n",
        "\n",
        "    model_dir_path = model_artifact.path\n",
        "    load_file_path = os.path.join(model_dir_path, MERIDIAN_MODEL_FILENAME)\n",
        "    logging.info(f\"Attempting to load model from file: {load_file_path}\")\n",
        "    if not os.path.exists(load_file_path):\n",
        "        raise FileNotFoundError(f\"Expected model file {MERIDIAN_MODEL_FILENAME} not found in {model_dir_path}\")\n",
        "    try:\n",
        "        mmm = model.load_mmm(load_file_path)\n",
        "        logging.info(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model loading failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "    if not output_gcs_dir.startswith(\"gs://\"):\n",
        "        raise ValueError(\"output_gcs_dir must be a GCS path (gs://...)\")\n",
        "    final_gcs_uri = os.path.join(output_gcs_dir, report_filename)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        logging.info(f\"Running optimization and generating report locally in: {temp_dir}\")\n",
        "        local_report_source_path = os.path.join(temp_dir, report_filename)\n",
        "        try:\n",
        "            budget_optimizer = optimizer.BudgetOptimizer(mmm)\n",
        "            optimization_results = budget_optimizer.optimize()\n",
        "            logging.info(\"Optimization calculation complete.\")\n",
        "            optimization_results.output_optimization_summary(\n",
        "                filename=report_filename,\n",
        "                filepath=temp_dir\n",
        "            )\n",
        "            logging.info(f\"Meridian saved optimization report locally to: {local_report_source_path}\")\n",
        "            if not os.path.exists(local_report_source_path):\n",
        "                 logging.error(f\"Meridian did not create the expected local report file: {local_report_source_path}\")\n",
        "                 raise FileNotFoundError(f\"Optimization report file not created locally by Meridian at {local_report_source_path}\")\n",
        "            logging.info(f\"Manually uploading {local_report_source_path} to {final_gcs_uri}\")\n",
        "            upload_local_file_to_gcs(local_report_source_path, final_gcs_uri)\n",
        "            optimization_report_artifact.uri = final_gcs_uri\n",
        "            optimization_report_artifact.metadata[\"gcs_path\"] = final_gcs_uri\n",
        "            optimization_report_artifact.metadata[\"filename\"] = report_filename\n",
        "            logging.info(f\"Set KFP artifact URI to: {optimization_report_artifact.uri}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed during budget optimization or reporting/uploading: {e}\")\n",
        "            raise e\n",
        "    logging.info(\"Optimization component finished.\")\n",
        "\n",
        "\n",
        "# --- Pipeline Definition ---\n",
        "@dsl.pipeline(\n",
        "    name=PIPELINE_NAME,\n",
        "    description=\"Runs Meridian MMM (GPU) reading from BigQuery\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def meridian_pipeline(\n",
        "    # --- REMOVE data_csv_url ---\n",
        "    # data_csv_url: str = DATA_CSV_URL,\n",
        "    # --- ADD BQ Parameters with defaults ---\n",
        "    project_id: str = PROJECT_ID,\n",
        "    bq_dataset: str = BQ_DATASET,\n",
        "    bq_table_name: str = BQ_TABLE_NAME,\n",
        "    # --- End BQ Parameters ---\n",
        "    output_gcs_dir: str = OUTPUT_GCS_DIR,\n",
        "    roi_mu: float = ROI_MU,\n",
        "    roi_sigma: float = ROI_SIGMA,\n",
        "    n_chains: int = N_CHAINS,\n",
        "    n_adapt: int = N_ADAPT,\n",
        "    n_burnin: int = N_BURNIN,\n",
        "    n_keep: int = N_KEEP,\n",
        "    seed: int = RANDOM_SEED,\n",
        "    report_start_date: str = REPORT_START_DATE,\n",
        "    report_end_date: str = REPORT_END_DATE,\n",
        "    summary_report_filename: str = \"summary_output.html\",\n",
        "    optimization_report_filename: str = \"optimization_output.html\",\n",
        "):\n",
        "    # Step : Train Model (pass BQ params)\n",
        "    train_task = train_meridian_model(\n",
        "        # --- Pass BQ params ---\n",
        "        project_id=project_id,\n",
        "        bq_dataset=bq_dataset,\n",
        "        bq_table_name=bq_table_name,\n",
        "        # --- End BQ params ---\n",
        "        roi_mu=roi_mu, roi_sigma=roi_sigma,\n",
        "        n_chains=n_chains, n_adapt=n_adapt, n_burnin=n_burnin, n_keep=n_keep, seed=seed,\n",
        "    )\n",
        "    train_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\")\n",
        "    train_task.set_accelerator_limit(1).set_accelerator_type('NVIDIA_TESLA_T4')\n",
        "\n",
        "    # Step : Generate Summary Report (no change needed here)\n",
        "    summary_task = generate_summary_report(\n",
        "        model_artifact=train_task.outputs[\"output_model\"],\n",
        "        output_gcs_dir=output_gcs_dir,\n",
        "        report_filename=summary_report_filename,\n",
        "        start_date=report_start_date,\n",
        "        end_date=report_end_date,\n",
        "    )\n",
        "    summary_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\") # Adjust if needed\n",
        "\n",
        "    # Step : Run Budget Optimization (no change needed here)\n",
        "    optimization_task = run_budget_optimization(\n",
        "        model_artifact=train_task.outputs[\"output_model\"],\n",
        "        output_gcs_dir=output_gcs_dir,\n",
        "        report_filename=optimization_report_filename,\n",
        "    )\n",
        "    optimization_task.set_cpu_limit(\"16\").set_memory_limit(\"64G\") # Adjust if needed\n",
        "\n",
        "\n",
        "# --- Pipeline Compilation and Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    kfp.compiler.Compiler().compile(\n",
        "        pipeline_func=meridian_pipeline, package_path=PIPELINE_JSON\n",
        "    )\n",
        "    print(f\"Pipeline compiled to {PIPELINE_JSON}\")\n",
        "\n",
        "    aip.init(project=PROJECT_ID, location=REGION, staging_bucket=PIPELINE_ROOT)\n",
        "    print(f\"Initialized Vertex AI SDK for project {PROJECT_ID} in {REGION}\")\n",
        "\n",
        "    job = aip.PipelineJob(\n",
        "        display_name=PIPELINE_NAME, # Use updated name\n",
        "        template_path=PIPELINE_JSON,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        parameter_values={\n",
        "            \"project_id\": PROJECT_ID,\n",
        "            \"bq_dataset\": BQ_DATASET,\n",
        "            \"bq_table_name\": BQ_TABLE_NAME,\n",
        "            \"output_gcs_dir\": OUTPUT_GCS_DIR,\n",
        "            \"roi_mu\": ROI_MU,\n",
        "            \"roi_sigma\": ROI_SIGMA,\n",
        "            \"n_chains\": N_CHAINS,\n",
        "            \"n_adapt\": N_ADAPT,\n",
        "            \"n_burnin\": N_BURNIN,\n",
        "            \"n_keep\": N_KEEP,\n",
        "            \"seed\": RANDOM_SEED,\n",
        "            \"report_start_date\": REPORT_START_DATE,\n",
        "            \"report_end_date\": REPORT_END_DATE,\n",
        "            \"summary_report_filename\": \"summary_output.html\",\n",
        "            \"optimization_report_filename\": \"optimization_output.html\",\n",
        "        },\n",
        "        enable_caching=False, # Caching to edit as desired\n",
        "    )\n",
        "\n",
        "    print(\"Submitting pipeline job...\")\n",
        "    job.submit()\n",
        "    print(f\"Pipeline job submitted. View in Cloud Console: {job._dashboard_uri()}\")"
      ],
      "metadata": {
        "id": "CHhoN5Cu3TfZ"
      },
      "id": "CHhoN5Cu3TfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4odjEd0-i3j"
      },
      "id": "V4odjEd0-i3j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Meridian-Pipeline-BQ-TO-GCS"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}