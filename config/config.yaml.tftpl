# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This *.tftpl file is a Terraform template file. It is used to create or update
# resources in a Google Cloud project. A Terraform template file contains the
# configuration parameters for the resources that you want to create or update.
# It also contains the logic for creating or updating the resources. Terraform
# template files are typically used to create or update resources that are
# complex or that require a lot of configuration.
#
# The config.yaml.tftpl file is a YAML file that contains all the configuration
# parameters for the marketing analytics jumpstart solution.

# It contains the following sections:
#
# google_cloud_project: This section contains the Google Cloud project ID and project number.
# cloud_build: This section contains the configuration parameters for the Cloud Build pipeline.
# container: This section contains the configuration parameters for the container images.
# artifact_registry: This section contains the configuration parameters for the Artifact Registry repository.
# dataflow: This section contains the configuration parameters for the Dataflow pipeline.
# vertex_ai: This section contains the configuration parameters for the Vertex AI pipeline.
# bigquery: This section contains the configuration parameters for the BigQuery artifacts.

# This block contains general configuration parameters for the Google Cloud Project.
google_cloud_project:
  project_id: "${project_id}" # project_id terraform variable hidrated by terraform.
  project_name: "${project_name}" # project name terraform variable hidrated by terraform.
  project_number: "${project_number}" # project number terraform variable hidrated by terraform.
  region: "${cloud_region}" # region terraform variable hidrated by terraform.

# This block contains configuration parameters for the Cloud Build steps.
# The CI/CD pipelines are going to use Cloud Build to validate and test the solution at any time the
# developer wants.
# The Cloud Build pipeline will be created via a Terraform resource.
# This capability is still under construction, it is not ready to use.
cloud_build:
  project_id: "${project_id}"
  region: "${cloud_region}"
  github:
    trigger_branch: "dev"
  build_file: "cloudbuild/pipelines.yaml"
  _REPOSITORY_GCP_PROJECT: "${project_id}"
  _REPOSITORY_BRANCH: "main"
  _GCR_HOSTNAME: "${cloud_region}-docker.pkg.dev"
  _BUILD_REGION: "${cloud_region}"

# This block contains configuration parameters for the container images.
# The CI/CD pipelines are going to use the container parameters to auxiliate the Docker containers
# creation.
# The container images will be created via a Terraform resource.
# This capability is still under construction, it is not ready to use.
container:
  builder:
    # This is the base image used to run linting, formatting and unit tests of the python code.
    base:
      from_image: "python:3.7-alpine3.7"
      base_image_name: "base-builder"
      base_image_prefix: "maj"
    # This is the zetasql formatter image used to format and validate the SQL queries.
    zetasql:
      from_image: "wbsouza/zetasql-formatter:latest"
      base_image_name: "zetasql-formatter"
      base_image_prefix: "maj"
  container_registry_hostname: "${cloud_region}-docker.pkg.dev"
  container_registry_region: "${cloud_region}"

# This block contains configuration parameters for the Artifact Registry repositories.
# The Pipelines terraform module uses Artifact Registry to store the pipelines YAML configuration files
# and the docker images.
# Two repositories are created: one for the pipelines and one for the docker images.
# The pipelines repository is used to store the pipelines YAML configuration files to be compiled
# and uploaded via a Terraform resource.
# The docker repository is used to store the Docker image built via a Terraform resource.
# The only image being built right now is the pipeline components container image.
artifact_registry:
  pipelines_repo:
    name: "pipelines-repo"
    region: "${cloud_region}"
    project_id: "${project_id}"
  pipelines_docker_repo:
    name: "pipelines-docker-repo"
    region: "${cloud_region}"
    project_id: "${project_id}"

# This block contains configuration parameters for the Dataflow jobs and templates.
# The Activation Application terraform module uses Dataflow templates to create the Dataflow job.
# The Dataflow job is responsible for sending the model predictions to the GA4 / GAds platforms via
# Measurement Protocol API.
dataflow:
  # The `worker_service_account_id` is the service account ID used by the dataflow workers.
  worker_service_account_id: "df-worker"
  # The `worker_service_account` is the service account used by the dataflow workers.
  # The service account is created via a Terraform resource.
  worker_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"

# This block contains configuration parameters for the Vertex AI pipeline components.
# The Pipelines terraform module uses Vertex AI pipeline components to create the Vertex AI pipeline.
# The Vertex AI pipeline uses a Docker image at every step which is customized for this solution.
# Navigate to Vertex AI -> Training -> Custom Job to see the pipeline components executions.
vertex_ai:
  # The components block contains configuration parameters for the Vertex AI pipeline components.
  components:
    # The `base_image_name` is the name of the Docker container image used by the pipeline components.
    # The base image is created via a Terraform resource.
    # The Dockerfile recipe can be found at `components/Dockerfile`.
    base_image_name: "meridian-cpu-base-image"
    base_image_tag: "dev"
    gpu_base_image_name: "meridian-gpu-base-image"
    gpu_base_image_tag: "dev"

  # This pipelines block contains configuration parameters for the Vertex AI pipelines.
  # The current pipelines are:
  # - meridian-pre-modeling
  # - meridian-modeling
  # - meridian-post-modeling
  pipelines:
    project_id: "${project_id}"
    service_account_id: "vertex-pipelines-sa"
    service_account: "vertex-pipelines-sa@${project_id}.iam.gserviceaccount.com"
    region: "${cloud_region}"
    bucket_name: "${project_id}-pipelines"
    model_bucket_name: "${project_id}-custom-models"
    root_path: "gs://${project_id}-pipelines/pipelines/"

    # This pipeline contains the configuration parameters for the pre-modeling stage of the Meridian model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources in infra/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in infra/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `pipelines/compiler.py` and `pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in `pipelines/[pipeline_name].py
    ## 9. Run terraform apply
    meridian-pre-modeling:
      execution:
        # The `name` parameter is the name of the pipeline that will appear in the Vertex AI pipeline UI.
        name: "meridian-pre-modeling"
        # The `job_id_prefix` is the prefix of the Vertex AI Custom Job that will be used at the execution of each individual component step.
        job_id_prefix: "meridian-pre-modeling-"
        # The `experiment_name` is the name of the experiment that will appear in the Vertex AI Experiments UI.
        experiment_name: "meridian-pre-modeling"
        # The `type` defines whether the pipeline is going to be a `tabular-workflows` or a `custom` pipeline.
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        # The `schedule` defines the schedule values of the pipeline.
        # This solution uses the Vertex AI Pipeline Scheduler.
        # More information can be found at https://cloud.google.com/vertex-ai/docs/pipelines/scheduler.
        schedule:
          # The `cron` is the cron schedule. Make sure you review the TZ=America/New_York timezone.
          # More information can be found at https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules.
          cron: "TZ=${time_zone} 0 1 * * *"
          # The `max_concurrent_run_count` defines the maximum number of concurrent pipeline runs.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `subnetwork` defines the subnetwork in which the pipeline will be executed.
          # The default value is "default".
          # Follow the guide: https://cloud.google.com/vertex-ai/docs/general/vpc-peering
          subnetwork: "default"
          # If you want to use the vpc network defined above, set the following flag to true
          use_private_service_access: false
          state: ${pipeline_configuration.meridian-pre-modeling.execution.schedule.state}
        # The `pipeline_parameters` defines the parameters that are going to be used to compile the pipeline.
        # Those values may difer depending on the pipeline type and the pipeline steps being used.
        # Make sure you review the python function the defines the pipeline.
        # The pipeline definition function can be found in `python/pipelines/feature_engineering_pipelines.py`
        # or other files ending with `python/pipelines/*_pipeline.py`.
        # Auto Audience Segmentation involved the dynamic identification of the most visited pages in the website by using the regular expression `reg_expression` and frequency percentual `perc_keep`.
        # This process takes into consideration a date interval defined by the parameters `date_start` and `date_end`.
        pipeline_parameters:
          #project_id: "${project_id}"
          #location: "${location}"
          #dataset: "auto_audience_segmentation"
          #feature_table: "page_path_cumulative_traffic"
          #date_start: "2024-01-01"
          #date_end: "2024-07-01"
          #mds_project_id: "${mds_project_id}"
          #mds_dataset: "${mds_dataset}"
          #stored_procedure_name: "auto_audience_segmentation_dataset_preparation"
          #full_dataset_table: "auto_audience_segmentation_full_dataset"
          #training_table: "auto_audience_segmentation_training_15"
          #inference_table: "auto_audience_segmentation_inference_15"
          # Increase the percentual `perc_keep` to include more pages in your segmentation, decrease to exclude pages in your segmentation
          #perc_keep: 50
          #query_auto_audience_segmentation_inference_preparation: "
          #  CALL `{auto_audience_segmentation_inference_preparation_procedure_name}`();"
          #query_auto_audience_segmentation_training_preparation: "
          #  CALL `{auto_audience_segmentation_training_preparation_procedure_name}`();"
          # The `timeout` parameter defines the timeout of the pipeline in seconds.
          # The default value is 3600 seconds (1 hour).
          #timeout: 3600.0
        # The `pipeline_parameters_substitutions` defines the substitutions that are going to be applied to the pipeline parameters before compilation.
        # Check the parameter values above to see if they are used.
        # They typically follow this format {parameter_subsititution_key}.
        # To apply a substitution, make sure you define the pair: {parameter_subsititution_key}: {parameter_subsititution_value}.
        #pipeline_parameters_substitutions:
        #  date_timezone: "UTC" # used when input_date is None and need to get current date.
        #  auto_audience_segmentation_inference_preparation_procedure_name: "${project_id}.auto_audience_segmentation.invoke_auto_audience_segmentation_inference_preparation"
        #  auto_audience_segmentation_training_preparation_procedure_name: "${project_id}.auto_audience_segmentation.invoke_auto_audience_segmentation_training_preparation"

    # This pipeline contains the configuration parameters for the modeling stage of the Meridian model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources in infra/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in infra/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `pipelines/compiler.py` and `pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in `pipelines/[pipeline_name].py
    ## 9. Run terraform apply
    meridian-modeling:
      execution:
        # The `name` parameter is the name of the pipeline that will appear in the Vertex AI pipeline UI.
        name: "meridian-modeling"
        # The `job_id_prefix` is the prefix of the Vertex AI Custom Job that will be used at the execution of each individual component step.
        job_id_prefix: "meridian-modeling-"
        # The `experiment_name` is the name of the experiment that will appear in the Vertex AI Experiments UI.
        experiment_name: "meridian-modeling"
        # The `type` defines whether the pipeline is going to be a `tabular-workflows` or a `custom` pipeline.
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        # The `schedule` defines the schedule values of the pipeline.
        # This solution uses the Vertex AI Pipeline Scheduler.
        # More information can be found at https://cloud.google.com/vertex-ai/docs/pipelines/scheduler.
        schedule:
          # The `cron` is the cron schedule. Make sure you review the TZ=America/New_York timezone.
          # More information can be found at https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules.
          cron: "TZ=${time_zone} 0 1 * * *"
          # The `max_concurrent_run_count` defines the maximum number of concurrent pipeline runs.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `subnetwork` defines the subnetwork in which the pipeline will be executed.
          # The default value is "default".
          # Follow the guide: https://cloud.google.com/vertex-ai/docs/general/vpc-peering
          subnetwork: "default"
          # If you want to use the vpc network defined above, set the following flag to true
          use_private_service_access: false
          state: ${pipeline_configuration.meridian-modeling.execution.schedule.state}
        # The `pipeline_parameters` defines the parameters that are going to be used to compile the pipeline.
        # Those values may difer depending on the pipeline type and the pipeline steps being used.
        # Make sure you review the python function the defines the pipeline.
        # The pipeline definition function can be found in `python/pipelines/feature_engineering_pipelines.py`
        # or other files ending with `python/pipelines/*_pipeline.py`.
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          query_user_segmentation_dimensions: "
            CALL `{user_segmentation_dimensions_procedure_name}`();"
          query_user_lookback_metrics: "
            CALL `{user_lookback_metrics_procedure_name}`();"
          query_audience_segmentation_inference_preparation: "
            CALL `{audience_segmentation_inference_preparation_procedure_name}`();"
          query_audience_segmentation_training_preparation: "
            CALL `{audience_segmentation_training_preparation_procedure_name}`();"
          # The `timeout` parameter defines the timeout of the pipeline in seconds.
          # The default value is 3600 seconds (1 hour).
          timeout: 3600.0
        # The `pipeline_parameters_substitutions` defines the substitutions that are going to be applied to the pipeline parameters before compilation.
        # Check the parameter values above to see if they are used.
        # They typically follow this format {parameter_subsititution_key}.
        # To apply a substitution, make sure you define the pair: {parameter_subsititution_key}: {parameter_subsititution_value}.
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          user_segmentation_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_segmentation_dimensions"
          user_lookback_metrics_procedure_name: "${project_id}.feature_store.invoke_user_lookback_metrics"
          user_scoped_segmentation_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_segmentation_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          audience_segmentation_inference_preparation_procedure_name: "${project_id}.audience_segmentation.invoke_audience_segmentation_inference_preparation"
          audience_segmentation_training_preparation_procedure_name: "${project_id}.audience_segmentation.invoke_audience_segmentation_training_preparation"

    # This pipeline contains the configuration parameters for the post modeling stage of the Meridian model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources in infra/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in infra/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `pipelines/compiler.py` and `pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in `pipelines/[pipeline_name].py
    ## 9. Run terraform apply
    meridian-post-modeling:
      execution:
        name: "meridian-post-modeling"
        job_id_prefix: "meridian-post-modeling-"
        experiment_name: "meridian-post-modeling"
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        schedule:
          cron: "TZ=${time_zone} 0 1 * * *"
          # Define the maximum number of concurrent pipeline runs.
          # The default value is 1.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `subnetwork` defines the subnetwork in which the pipeline will be executed.
          # The default value is "default".
          # Follow the guide: https://cloud.google.com/vertex-ai/docs/general/vpc-peering
          subnetwork: "default"
          # If you want to use the vpc network defined above, set the following flag to true
          use_private_service_access: false
          state: ${pipeline_configuration.meridian-post-modeling.execution.schedule.state}
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          # The query_purchase_propensity_label defines the procedure that will be used to invoke the creation of the purchase propensity label feature table.
          query_purchase_propensity_label: "
            CALL `{purchase_propensity_label_procedure_name}`();"
          # The query_user_dimensions defines the procedure that will be used to invoke the creation of the user dimensions feature table.
          query_user_dimensions: "
            CALL `{user_dimensions_procedure_name}`();"
          # The query_user_rolling_window_metrics defines the procedure that will be used to invoke the creation of the user rolling window metrics feature table.
          query_user_rolling_window_metrics: "
            CALL `{user_rolling_window_metrics_procedure_name}`();"
          # The query_purchase_propensity_inference_preparation define the procedure that will be used to invoke the creation of the purchase propensity inference preparation table.
          query_purchase_propensity_inference_preparation: "
            CALL `{purchase_propensity_inference_preparation_procedure_name}`();"
          # The query_purchase_propensity_training_preparation define the procedure that will be used to invoke the creation of the purchase propensity training preparation table.
          query_purchase_propensity_training_preparation: "
            CALL `{purchase_propensity_training_preparation_procedure_name}`();"
          timeout: 3600.0
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          purchase_propensity_label_procedure_name: "${project_id}.feature_store.invoke_purchase_propensity_label"
          user_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_dimensions"
          user_rolling_window_metrics_procedure_name: "${project_id}.feature_store.invoke_user_rolling_window_metrics"
          user_scoped_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_metrics"
          user_session_event_aggregated_metrics_procedure_name: "${project_id}.feature_store.invoke_user_session_event_aggregated_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          purchase_propensity_inference_preparation_procedure_name: "${project_id}.purchase_propensity.invoke_purchase_propensity_inference_preparation"
          purchase_propensity_training_preparation_procedure_name: "${project_id}.purchase_propensity.invoke_purchase_propensity_training_preparation"

# This block contains configuration parameters for the BigQuery Datasets, Tables, Queries and Stored Procedures.
bigquery:
  project_id: "${project_id}"
  region: "${location}"
  dataset:
    # Dataset for the feature engineering tables and procedures.
    meridian_modeling:
      project_id: "${project_id}"
      name: "meridian_modeling"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Feature Store dataset for Marketing behavioural modeling"
      friendly_name: "Feature Store"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    meridian_output:
      project_id: "${project_id}"
      name: "meridian_output"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Feature Store dataset for Marketing behavioural modeling"
      friendly_name: "Feature Store"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
  query:
    invoke_meridian_modeling_training_preparation:
      project_id: "${project_id}"
      dataset: "meridian_modeling"
      stored_procedure: "meridian_modeling_training_preparation"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      interval_min_weeks: 104
      interval_max_weeks: 180
  procedure:
    meridian_modeling_training_preparation:
      project_id: "${project_id}"
      dataset: "meridian_modeling"
      name: "meridian_modeling_training_preparation"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      table_name: "meridian_modeling_training_preparation_full_dataset"
      view_name: "meridian_modeling_training_preparation"
      expiration_duration_hours: 168
      # This is the datetime column name.
      datetime_column: "Dt"
      # Change the `eval_start_date` and `eval_end_date` in case you need your model to be validated in another interval of time.
      # The explanation metrics will be generated based on this subset.
      train_start_date: "2022-01-01"
      train_end_date: "2025-03-31"


